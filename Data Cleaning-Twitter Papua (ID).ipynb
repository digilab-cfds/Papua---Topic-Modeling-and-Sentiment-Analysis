{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Twitter - Papua _ID_cleaning data2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8hM-_ZGe9Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEJlB4Yker92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_inter = pd.read_excel('Twitter ID Interactions.xlsx')\n",
        "data_post = pd.read_excel('Twitter ID Posts.xlsx')\n",
        "data_inter.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSOA7CggewuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts_inter = data_inter['Post']\n",
        "posts_post = data_post['Post']\n",
        "print('Data Twitter EN Interactions : ',len(posts_inter))\n",
        "print('Data Twitter EN Posts : ', len(posts_post))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VhVjTf2llY7",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning without stemming and stopwords removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiG-C4urgBkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "d = {}\n",
        "with open(\"formalization.txt\") as f:\n",
        "    for line in f:\n",
        "        (key, val) = line.split(\"\\t\")\n",
        "        d[key] = re.sub('\\n','',val)\n",
        "def slang(sen,d):\n",
        "    final = ' '.join(str(d.get(word,word)) for word in sen.split())\n",
        "    return final\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9_]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "pat3 = r'RT'\n",
        "pat4 = r'#[A-Za-z0-9_]+'\n",
        "#remove link gambar\n",
        "pat5 = r'pic.twitter.com/[A-Za-z0-9]+'\n",
        "combined_pat = r'|'.join((pat1,pat2,pat3,pat4,pat5))\n",
        "#stop_words = set(stopwords.words('indonesian'))\n",
        "#factory = StemmerFactory()\n",
        "#stemmer = factory.create_stemmer()\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    \n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    \n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "    \n",
        "    letters_only = re.sub(\"[^a-zA-Z]\",\" \", clean)\n",
        "    \n",
        "    lower_case = letters_only.lower()\n",
        "    #handling slang words\n",
        "    slang_word = slang(lower_case,d)\n",
        "    \n",
        "    #stem_words = stemmer.stem(slang_word)\n",
        "    \n",
        "    #translate to ind\n",
        "    \n",
        "    words = tok.tokenize(slang_word)\n",
        "     \n",
        "    #filtered_words = [w for w in words if not w in stop_words]\n",
        "    '''\n",
        "    print(\"souped : \", souped)\n",
        "    print(\"\\nstripped : \", stripped)\n",
        "    print(\"\\nclean : \", clean)\n",
        "    print(\"\\nletters_only : \", letters_only)\n",
        "    print(\"\\nslang word : \",slang_word)\n",
        "    print(\"\\nstem_words : \", stem_words)\n",
        "    print(\"\\nwords tokenized: \", words)\n",
        "    '''\n",
        "    #return(\" \".join(filtered_words)).strip()\n",
        "    return(\" \".join(words)).strip()\n",
        "    #return(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTC6SM5Jgq5C",
        "colab_type": "code",
        "outputId": "f16c5e56-8d98-4b3c-bed2-e136443e21bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "clean_tweets_post = [tweet_cleaner(posts_post[i]) for i in range(len(posts_post))]\n",
        "\n",
        "print('DONE POSTS!')\n",
        "'''\n",
        "with tqdm(total=len(posts_inter)) as pbar:\n",
        "  clean_tweets_inter = []\n",
        "  for i in range(len(posts_inter)):\n",
        "    clean_tweets_inter.append(tweet_cleaner(posts_inter[i]))\n",
        "    pbar.update()\n",
        "  \n",
        "print('DONE INTERACTIONS!')\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/0oc3nYjL0X\n",
            "#lima-mahasiswa-papua-di-ponorogo-hadiri-pembukaan-grebeg-suro\n",
            "#times_indonesia\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/sqNnuqOVvc\n",
            "\n",
            "Teruuussss....???\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/f6U2RRNZxg\n",
            "Mahasiswa?\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DONE POSTS!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nwith tqdm(total=len(posts_inter)) as pbar:\\n  clean_tweets_inter = []\\n  for i in range(len(posts_inter)):\\n    clean_tweets_inter.append(tweet_cleaner(posts_inter[i]))\\n    pbar.update()\\n  \\nprint('DONE INTERACTIONS!')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvdL_QRSj3ot",
        "colab_type": "code",
        "outputId": "8c2f5e05-cbac-4593-b26f-10180a7978d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "clean_tweets_inter[:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['masyarat papua di tanah jawa cuma sedikit jika diusir dari sn tidak masalah tanah dan hutan kami luas jika kami laku hal sebaliknya apa tidak bertmbh jumlah gelandangan di jawa',\n",
              " 'redaksi tolong baca dan minta maaf kepada mahasiswa papua yang anda tulis beritanya kemarin media sampah media provokator',\n",
              " 'alasan polisi karena perusakan bendera merah putih kenapa harus dikepung iya kan bisa dengan cara baik mengirimkan surat panggilan untuk diperiksa penyidik kalau begini caranya seperti mau menangkap teroris',\n",
              " 'kenapa tidak memuat berita hari ini min',\n",
              " 'kalau merasa indonesia memang membawa kemajuan di papua kan nanti menang di referendum berarti tidak percaya diri iya',\n",
              " 'hadir di kalasan asrama papua anda bung warga surabaya sudah mengutamakan musyawarah meskipun bendera tergeletak di got kalau tidak hadir diam biar tidak tambah gaduh',\n",
              " 'muke kamu pade serem ditodong gas aer mata begitu saja angkat tangan kalah kamu sama boca tanah abang yang kemarin tempur hari malam batu vs senapan serbu tidak ada yang angkat tangan',\n",
              " 'dapat dari twit indoprogres dan ini menarik belum lagi terakhir ramai hinaan hinaan seperti monyet dan sebagainya terlontarkan ke teman teman asrama papua jahatnya kok tidak pada disaring iya ngalir begitu saja',\n",
              " 'ini kenapa sih siapa yang bilang begitu memang ada antara memang beneran tidak tahu kejadiannya atau sekedar denial saja sm rasisme terhadap orang papua saya tidak mengerti deh',\n",
              " 'sedang apa acab menangkap teroris tidak mereka hanyalah mahasiswa tak bersenjata yang dituduh',\n",
              " 'mudah mudahan saja masyarakat papua yang ada di papua tidak mengusir pendatang baru yang ada di daerahnya',\n",
              " 'hmm iya benar tapi bukti kalau penghuni asrama tersebut yang sudah dengan segaja merusak bendera dan membuangnya di depan asrama mereka itu tidak kuat bila hanya berdasarkan foto yang tersebar di grup wa tidak ada yang tahu kejadian sebenarnya seperti apa seharusnya diselidiki dulu',\n",
              " 'itulah sop nya mengantar memberi minum adalah melanggar bisa contoh yang sudah terjadi pada karyawan sarinah di jakarta memberi air pada pendemo yang kena gas air mata di pidanakan',\n",
              " 'apa karena merah putih maka kalian patahkan tiangnya dan kalian campakkan ke tanah',\n",
              " 'saat petugas yang menjadi korban semua dengan tampang baik bersimpati tapi saat mahasiswa yang berjuang untuk keadilan adakah simpati panjang umur perjuangan tetap kuat saudara ku tetap tegak berdiri',\n",
              " 'referendum sekarang juga surabaya java ini yel yel massa sepanjang malam usir usir papua usir papua sekarang juga mob rounding up west papuan students dorm were singing this all night kick out kick out papua kick out papua right now they are still trapped inside',\n",
              " 'kami datang ke jawa sini bukan untuk cari kerja bukan untuk cari makan kami hanya numpang kuliah saja sementara orang orang yang datang dari sini ke papua mereka cari kerja dan cari makan di tanah kami tapi kami tidak rasis seperti itu biasa biasa saja miris memang',\n",
              " 'kenapa lagunya hampir sama bunuh bunuh si ahok seperti ada yang bermain disini sekali lagi oknum usir jawa usir papua usir ini itu yang tidak berbuat kenapa di ikutkan',\n",
              " 'tidak juga ini belum bicara kekerasan di timor timur aceh dulu',\n",
              " 'kita selalu memberhalakan sesuatu yang simbolis bendera koyak dianggap tidak indonesia upaya makar nasionalisme sempit semacam ini yang paling berbahaya']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omtox2mgj7Fw",
        "colab_type": "code",
        "outputId": "f3dd6586-16d4-48f9-b4c7-4b7b8ce1e5d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "clean_tweets_post[:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['agustus rakyat vanuatu melakukan demonstrasi mendukung ulmwp menjadi anggota penuh di msg wp msg ulmwp yes west papua freedom',\n",
              " 'ini pemicu kerusuhan yang terjadi di manokwari hari ini pengepungan asrama papua dipicu foto di grup whatsapp',\n",
              " 'akun provokator seperti masih bebas saja mencuit memprovokasi cc',\n",
              " 'kita tunggu reaksi para pengklaim garda terdepan nkri semacam',\n",
              " 'seperti suara yang jauh di mana kabar tentang kekerasan dan kekayaan alam selalu terdengar sayup sayup',\n",
              " 'beer kalau di suatu konflik ada pihak fp yang ikutan saya yakin isu yang didengungkan tidak benar mahasiswa p pu dikambinghitamkan atas kepentingan beberapa pihak besar yang mungkin tidak mau papua bersatu atau di balik ini ada kasus yang mau ditutupi dengan cara licik sprt ini',\n",
              " 'noda hitam di tahun ini segala bentuk makar ancaman keutuhan nkri harus ditindak tegas',\n",
              " 'kondisi di papua barat jalan di blokade sejumlah bangunan dibakar penembakan suasana mencekam',\n",
              " 'kmrnn sore jam wib kami kaget saat tni mendobrak pintu disertai hai anjing babi monyet keluar kamu kalau berani hadapi kami di depan ujarnya kisah penangkapan orang di asrama papua surabaya versi mahasiswa via',\n",
              " 'biadab fpi yang maki mahasiswa papua di surabaya dengan monyet babi',\n",
              " 'kondisi manokwari papua barat sudah kondusif massa dikabarkan sudah bubar dari jalan titik berkumpul massa',\n",
              " 'gubernur jatim minta maaf pada rakyat papua atas insiden di malang dan surabaya begitu teks di tv salut ibu gub ayo bapak yang lain minta maaflah atas kekhilafan yang ada agar masalah tak berkepanjangan bangsa besar adalah bangsa pemaaf',\n",
              " 'asrama papua di surabaya digeruduk massa beratribut fpi cnn indonesia',\n",
              " 'anggota komisi iii dpr ri supiadin aries saputra menyatakan prihatinan atas kerusuhan di manokwari papua barat supiadin mengimbau masyarakat untuk dapat menahan diri dan tidak bertindak anarkistis selengkapnya',\n",
              " 'papua mendapat perlakuan rasis meanwhile sda nya dikeruk kalian yang rasis memang perlu dikutuk racism isn t cute stupid people',\n",
              " 'si habib palsu novel bamukmin na udzubillah provokator fpi tuntut tni berlakukan darurat militer di papua',\n",
              " 'massa menuntut jaminan keamanan mahasiswa asal papua di wilayah jawa serta menuntut adanya permintaan maaf terkait pernyataan pejabat soal mahasiswa papua',\n",
              " 'tidak nyangka teman kerja ada yang rasis soal papua she said orang papuanya diamankan karena suka mabuk mabukan terus kadang suka buat masalah dan meresahkan mabok bikin masalah itu tidak perlu point at papuanya orang kalau maboknya resek iya bakal meresahkan iya ngeganggu',\n",
              " 'kalian orang papua boleh minta apa saja asal jangan minta merdeka demikian kata gus dur bisakah jokowi meneladani gus dur dengan menarik personil militer misalnya',\n",
              " 'jika saudara kita di papua monyet saya pun juga monyet karena papua adalah saudara saya alam mereka untuk kesejahteraan negeri ini tapi kenapa mereka tidak merasa aman di negeri mereka sendiri papua juga indonesia']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFLUDh3yjPOU",
        "colab_type": "code",
        "outputId": "b345402a-36ca-43b0-cbc5-39e9ef8823fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Jumlah Tweet ID Interaction : \",len(clean_tweets_inter))\n",
        "clean_tweets_remove_dups = list(dict.fromkeys(clean_tweets_inter))\n",
        "print(\"Jumlah Tweet ID Interaction tanpa duplikat : \", len(clean_tweets_remove_dups))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jumlah Tweet ID Interaction :  1024145\n",
            "Jumlah Tweet ID Interaction tanpa duplikat :  140035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVwaWnE4jhsm",
        "colab_type": "code",
        "outputId": "33311f00-3197-40e9-de3f-fb40c3fe67d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Jumlah Tweet ID Post : \",len(clean_tweets_post))\n",
        "clean_tweets_remove_dups2 = list(dict.fromkeys(clean_tweets_post))\n",
        "print(\"Jumlah Tweet ID Post tanpa duplikat : \", len(clean_tweets_remove_dups2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jumlah Tweet ID Post :  92749\n",
            "Jumlah Tweet ID Post tanpa duplikat :  60315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PG7HgWs1NF4",
        "colab_type": "text"
      },
      "source": [
        "## DATA TWITTER BAHASA INDONESIA\n",
        "\n",
        "- Jumlah Tweet ID interactions : 1.024.145\n",
        "- Jumlah Tweet tanpa duplikat  : 140.035\n",
        "\n",
        "\n",
        "- Jumlah Tweet ID Posts       : 92.749\n",
        "- Jumlah Tweet tanpa duplikat : 60.315\n",
        "\n",
        "\n",
        "- Jumlah Tweet ID             : 1.116.894\n",
        "- Jumlah Tweet tanpa duplikat : 200.350"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn9x8e1eoFxf",
        "colab_type": "text"
      },
      "source": [
        "## Stemming and Stopwords Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNiamgGpoEQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "tok = WordPunctTokenizer()\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "def tweet_cleaner2(text):\n",
        "  \n",
        "  stem_words = stemmer.stem(text)\n",
        "  words = tok.tokenize(stem_words)\n",
        "  filtered_words = [w for w in words if not w in stop_words]\n",
        " \n",
        "  return(\" \".join(filtered_words)).strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYiq09e9rjR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "'''\n",
        "with tqdm(total=len(clean_tweets_inter)) as pbar:\n",
        "  clean_tweets_inter2 = []\n",
        "  for i in range(len(clean_tweets_inter2),len(clean_tweets_inter)):\n",
        "    clean_tweets_inter2.append(tweet_cleaner2(clean_tweets_inter[i]))\n",
        "    pbar.update()\n",
        "'''  \n",
        "clean_tweets_post2 = [tweet_cleaner2(clean_tweets_post[i]) for i in range(len(clean_tweets_post))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY9FEmBtsYqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm\n",
        "'''\n",
        "with tqdm(total=len(clean_tweets_remove_dups)) as pbar:\n",
        "  clean_tweets_inter_remove_dups = []\n",
        "  for i in range(len(clean_tweets_remove_dups)):\n",
        "    clean_tweets_inter_remove_dups.append(tweet_cleaner2(clean_tweets_remove_dups[i]))\n",
        "    time.sleep(0.1)\n",
        "    pbar.update(10)\n",
        "''' \n",
        "clean_tweets_post_remove_dups = [tweet_cleaner2(clean_tweets_remove_dups2[i]) for i in range(len(clean_tweets_remove_dups2))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J1kbC6ljq6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clean_id_interactions = pd.DataFrame(clean_tweets_inter2)\n",
        "clean_id_posts = pd.DataFrame(clean_tweets_post2)\n",
        "\n",
        "#clean_inter_remove_dups = pd.DataFrame(clean_tweets_inter_remove_dups)\n",
        "clean_post_remove_dups = pd.DataFrame(clean_tweets_post_remove_dups)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwfbAM98kZPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clean_id_interactions.to_csv('clean_ID_interactions.csv', index=False, header=['Post'])\n",
        "clean_id_posts.to_csv('clean_ID_posts.csv', index=False, header=['Post'])\n",
        "\n",
        "#clean_inter_remove_dups.to_csv('clean_ID_interactions_remove_dups.csv', index=False, header=['Post'])\n",
        "clean_post_remove_dups.to_csv('clean_ID_posts_remove_dups.csv', index=False, header=['Post'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVypOPO1k1QB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "#files.download('clean_ID_interactions.csv')\n",
        "files.download('clean_ID_posts.csv')\n",
        "#files.download('clean_ID_interactions_remove_dups.csv')\n",
        "files.download('clean_ID_posts_remove_dups.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMZgZ6ssmI1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}